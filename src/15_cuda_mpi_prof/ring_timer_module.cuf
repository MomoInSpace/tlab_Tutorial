module RING_TIMER_MODULE
    use TLAB_CONSTANTS, only: wp
    use TLAB_ARRAYS
    use cudafor
    use mpi_f08
    use EXPORT_ARRAYS
    implicit none

contains

    !subroutine check_shapes(x_mat, y_mat, z_mat)
    !    real(wp), dimension(:,:),intent(in)  :: x_mat
    !    real(wp), dimension(:,:),intent(in)  :: y_mat
    !    real(wp), dimension(:,:),intent(in)  :: z_mat
    
        ! Check size of input arrays
    !    if(size(x_mat,dim=2) /= size(y_mat,dim=1)) stop "x and y input array size not match"

        ! Check if z_mat has the right size
    !    if(size(x_mat,dim=1) /= size(z_mat,dim=1)) stop "x and z input array size not match"
    !    if(size(y_mat,dim=2) /= size(z_mat,dim=2)) stop "y and z input array size not match"
        
    !end subroutine check_shapes

    subroutine ring_cpu(dim1,dim2)
        INTEGER :: my_rank, size, dim1, dim2
        INTEGER :: right, left
        INTEGER :: index1, index2, i,j,k
        INTEGER :: rank_printer = 1
        real(wp) :: sum
        !INTEGER, ASYNCHRONOUS :: snd_buf
        !INTEGER, managed :: snd_buf
        !INTEGER :: rcv_buf
        TYPE(MPI_Status)  :: status
        TYPE(MPI_Request) :: request
        real(wp) :: sum_value = 0

        ! GPU Test:
        !real , device :: x_dev(dim1, dim2), y_dev(dim1, dim2), z_dev(dim1, dim2)     ! Grid and associated arrays
        !x_dev = x
        !y_dev = y
        !z_dev = z



        ! MPI ------------------------------------------------------------------------------

        CALL MPI_Comm_rank(MPI_COMM_WORLD, my_rank)
        CALL MPI_Comm_size(MPI_COMM_WORLD, size)
        right = mod(my_rank+1,      size)
        left  = mod(my_rank-1+size, size)


        if (my_rank == size-1) then
        do index1 = 1, dim1
            do index2 = 1, dim2
                if ( index1==index2 ) then
                    y(index1,index2) = my_rank + 1
                end if
            end do
        end do
        end if

        DO index1 = 1, size

        !if (my_rank == rank_printer) then
        !    write(*,*) "Step", index1, " before calc"
        !    write( * , " (*(g0.1))" )  x 
        !    write( * , " (*(g0.1))" )  y 
        !    write( * , " (*(g0.1))" )  z 
        !end if


            do j=1,dim2
                do i=1,dim1
                    sum_value = 0
                    do k=1, dim2
                        sum_value = sum_value + x(i,k)*y(k,j)
                    end do      
                    z(i,j)= sum_value 
                end do
            end do


        !if (my_rank == rank_printer) then
        !    write(*,*) "Step", index1, " after calc"
        !    write( * , " (*(g0.1))" )  x 
        !    write( * , " (*(g0.1))" )  y 
        !    write( * , " (*(g0.1))" )  z 
        !end if


            CALL MPI_Irecv  (x, dim1*dim2, MPI_DOUBLE, left,  17, MPI_COMM_WORLD, request)
            CALL MPI_Ssend(z, dim1*dim2, MPI_DOUBLE, right, 17, MPI_COMM_WORLD)
            CALL MPI_Wait(request, status)
            IF (.NOT.MPI_ASYNC_PROTECTS_NONBLOCKING) CALL MPI_F_sync_reg(x)

        !if (my_rank == rank_printer) then
        !    write(*,*) "Step", index1, " after send"
        !    write( * , " (*(g0.1))" )  x 
        !    write( * , " (*(g0.1))" )  y 
        !    write( * , " (*(g0.1))" )  z 
        !    write(*,*) " "
        !end if


        END DO

        if (my_rank == rank_printer) then

              sum = x(1,1)
              WRITE(*,*) "PE", my_rank, ": Sum =", sum
        end if


    end subroutine ring_cpu 

end module
