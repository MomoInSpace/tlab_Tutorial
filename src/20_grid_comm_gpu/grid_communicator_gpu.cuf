module grid_comm_gpu_module
    use mpi_f08
    use grid_handler
    use grid_handler_gpu
    use grid_comm_module
    implicit none

    type, extends(Grid3D_Comm_Handler):: Grid3D_Comm_gpu
        contains
        procedure:: rotate_grid
    end type

    interface Grid3D_Comm_gpu
        module procedure :: createGrid3D_Comm_gpu
    end interface

    contains

    class(Grid3D_Comm_gpu) &
         function createGrid3D_Comm_gpu( world_size,     &
                          block_xyz_dims, &
                          block_multiplication_xyz_state, &
                          column_upper_limit) result(self)

        ! Parameters============================================================
        class(Grid3D_Comm_gpu):: self
        integer, intent(in)       :: world_size
        integer                   ::  rank, vertical_dimension, max_area
        integer, dimension(2):: task_dims  
        INTEGER, intent(in), &
                 dimension(3) :: block_xyz_dims, &
                                ! Describes the size of the smallest grid_unit.
                                block_multiplication_xyz_state
                                ! The block_xyz_dims get multiplied by the 
                                ! threads as indicated by this state.
                                ! Depending on the communication algorithm used, 
                                ! the programm needs different states!

    INTEGER,       dimension(3) ::  subgrid_xyz_dims
                                ! The size of the subgrid for each thread as 
                                ! indicated by block_multiplication_xyz_state
        integer, intent(in)  :: column_upper_limit
        logical, dimension(2):: periods = [.false., .false.]
        ! Error integer:
        integer, dimension(100):: ierr  = 0
        ! integer, dimension(3), intent(out):: subgrid_xyz_dims

        ! Body==================================================================
        self%Grid3D_Comm_Handler = Grid3D_Comm_Handler( world_size,     &
                          block_xyz_dims, &
                          block_multiplication_xyz_state, &
                          column_upper_limit)

    end function createGrid3D_Comm_gpu

    subroutine rotate_grid(self, grid_handler_send, grid_handler_rcv, pertubation, grid_handler_tmp)
    use communication_parameter
    ! IO =======================================================================
    class(Grid3D_Comm_gpu)         :: self
    type(Grid3D_gpu), target, & 
                      intent(inout)    :: grid_handler_send, grid_handler_rcv
    type(Grid3D_gpu), target, &
                      intent(inout) :: grid_handler_tmp
    integer, dimension(3), intent(in)  :: pertubation
    ! Parameters================================================================
    ! Cuda aware parameters
    real(kind = wp), pointer, &
                     device, &
                     dimension(:)    :: send_buf_pointer_gpu => null(), &
                                        work_space_send_gpu => null(), &
                                        work_space_tmp_gpu => null(), &
                                        stencil_send_gpu => null()
    real(kind = wp), pointer, &
                     device, &
                     dimension(:,:,:):: work_space3D_send_gpu, &
                                        work_space3D_tmp_gpu, &
                                        grid3D_pointer_send_gpu, &
                                        grid3D_pointer_rcv_gpu, &
                                        grid3D_pointer_tmp_gpu

    ! MPI_PARAMS                                
    TYPE(MPI_Request), dimension(:), &
                       managed, &
                       pointer :: GRID_COMM_REQUESTS_tmp
    TYPE(MPI_Status), dimension(:), &
                      managed, &
                      pointer  :: GRID_COMM_STATUS_tmp

    call self%rotate_grid_setup(grid_handler_send, grid_handler_rcv, pertubation, grid_handler_tmp)

    ! Communication loop----------------------------------------------------

    call grid_handler_send%get_switch_dims_workspace( &
            dims_send, &
            work_space3D_send_gpu, &
            work_space_send_gpu, & ! Could Be Empty TODO
            grid3D_pointer_send_gpu, &
            pertubation)

    call grid_handler_tmp%get_switch_dims_workspace( &
            dims_tmp, &
            work_space3D_tmp_gpu, &
            work_space_tmp_gpu, &
            grid3D_pointer_tmp_gpu, &
            pertubation)

    call grid_handler_rcv%get_pointer_3D(grid3D_pointer_rcv_gpu)

    

    ierr = 0
    if (pertubation(1) == 2) call rotate_213()
    if (pertubation(1) == 3) call rotate_321_tmp()
    !    if (present(grid_handler_tmp)) then
    !        !write(*,*) "TMP"
    !        call rotate_321_tmp() 
    !    else
    !        !write(*,*) "no TMP"
    !        !call rotate_321()
    !    end if
    !end if

    ! Cleanup--------------------------------------------------------------
    if (any(ierr /= MPI_SUCCESS)) error stop "ERROR in MPI_Igather" 
    if (allocated(ierr)) deallocate(ierr, stat = ierr0)
    if (ierr0 /= 0) print *, "ierr: Deallocation request denied rotate_grid 1"

    if (allocated(rcv_j)) deallocate(rcv_j, stat = ierr0)
    if (ierr0 /= 0) print *, "ierr: Deallocation request denied rotate_grid 3"
    
    if (allocated(m_max)) deallocate(m_max, stat = ierr0)
    if (ierr0 /= 0) print *, "ierr: Deallocation request denied rotate_grid 3"

    if (allocated(root)) deallocate(root, stat = ierr0)
    if (ierr0 /= 0) print *, "ierr: Deallocation request denied rotate_grid 3"

    contains 

    subroutine rotate_321_tmp()
        ! Body======================================================================
        !deviceptr(x_dev, y_dev, z_dev, my_rank_dev)

        !$acc data deviceptr(work_space3D_tmp_gpu, grid3D_pointer_send_gpu)
        !$acc parallel loop async
        do k = 1, dims_rcv(3) 
            !$acc loop independent
            do j = 1, dims_rcv(2) 
                !$acc loop independent
                do i = 1, dims_rcv(1) 
                    work_space3D_tmp_gpu(i, j, k) =  grid3D_pointer_send_gpu(k, j, i)
                end do

                !call MPI_Igather(SENDBUF   = work_space3D_tmp_gpu(:,j,k), &
                !                sendcount = send_count, &
                !                sendtype  = MPI_DOUBLE, &
                !                recvbuf   = grid3D_pointer_rcv_gpu(:, j, rcv_j(k)), &
                !                recvcount = send_count, &
                !                recvtype  = MPI_DOUBLE, &
                !                root      = root(k), &
                !                comm      = self%MPI_Comm_Column, &
                !                request   = grid_handler_rcv%GRID_COMM_REQUESTS(j+dims_rcv(2)*(k-1)), &
                !                ierror    = ierr(j,k))
            end do
        end do
        !$acc end parallel
        !$acc wait 
        !$acc end data 

        do k = 1, dims_rcv(3) 
            do j = 1, dims_rcv(2) 
            call MPI_Igather(SENDBUF   = work_space3D_tmp_gpu(:,j,k), &
                            sendcount = send_count, &
                            sendtype  = MPI_DOUBLE, &
                            recvbuf   = grid3D_pointer_rcv_gpu(:, j, rcv_j(k)), &
                            recvcount = send_count, &
                            recvtype  = MPI_DOUBLE, &
                            root      = root(k), &
                            comm      = self%MPI_Comm_Column, &
                            request   = grid_handler_rcv%GRID_COMM_REQUESTS(j+dims_rcv(2)*(k-1)), &
                            ierror    = ierr(j,k))
            end do
        end do

        ! Deallocation of request is done in MPI_WaitAll
        !ierr0 = 0
        !call MPI_WaitAll(dims_rcv(2)*dims_rcv(3), GRID_COMM_REQUESTS, GRID_COMM_STATUS, ierr0)
        !if (ierr0 /= MPI_SUCCESS) error stop "Grid Row 321 tmp Failed"
        !call MPI_Barrier(MPI_Comm_World, ierr0)
        !call MPI_Comm_rank(MPI_COMM_WORLD, ierr0)
        !if (ierr0 ==0) then
        !    write(*,*) "rotate 321", dims_rcv(1), dims_rcv(2), dims_rcv(3)
        !    dims_rcv = grid_handler_rcv%get_dims()
        !    write(*,*) "getdims 321", dims_rcv(1), dims_rcv(2), dims_rcv(3)
        !end if 
        !call MPI_Barrier(MPI_Comm_World, ierr0)
        !call self%grid_waitall(grid_handler_rcv)
        !call MPI_Barrier(MPI_Comm_World, ierr0)

    end subroutine rotate_321_tmp


    subroutine rotate_213()
        ! Body======================================================================
        !call system_clocK(m)
        
        !$acc data deviceptr(work_space3D_tmp_gpu, grid3D_pointer_send_gpu)
        !$acc parallel loop async
        do k = 1, dims_rcv(3) 
            !$acc loop independent
            do j = 1, dims_rcv(2) 
                !$acc loop independent
                do i = 1, dims_rcv(1) 
                    work_space3D_tmp_gpu(i,j, k) =  grid3D_pointer_send_gpu(j, i, k)
                end do
                
                !call MPI_Igather(sendbuf   = work_space3D_tmp_gpu(:,j, k), &
                !                sendcount = send_count, &
                !                sendtype  = MPI_DOUBLE, &
                !                recvbuf   = grid3D_pointer_rcv_gpu(:,rcv_j(j), k), &
                !                recvcount = send_count, &
                !                recvtype  = MPI_DOUBLE, &
                !                root      = root(j), &
                !                comm      = self%MPI_Comm_Row, &
                !                !request   = request(j, k), &
                !                request   = grid_handler_rcv%GRID_COMM_REQUESTS(j+dims_rcv(2)*(k-1)), &
                !                ierror    = ierr(j,k))
                !                !request(dims_rcv(2), dims_rcv(3)) => GRID_COMM_REQUESTS

            end do
        end do
        !$acc end parallel
        !$acc wait 
        !$acc end data 
        
        !call system_clocK(n)
        !if (my_rank == 0) write(*,*) "213", "Send Time Calc:",  (n-m)

        !if (my_rank == 0) write(*,*) "rcv_j", rcv_j
        !if (my_rank == 0) write(*,*) "root", root

        do k = 1, dims_rcv(3) 
            do j = 1, dims_rcv(2) 
                call MPI_Igather(sendbuf   = work_space3D_tmp_gpu(:,j, k), &
                                sendcount = send_count, &
                                sendtype  = MPI_DOUBLE, &
                                recvbuf   = grid3D_pointer_rcv_gpu(:,rcv_j(j), k), &
                                recvcount = send_count, &
                                recvtype  = MPI_DOUBLE, &
                                root      = root(j), &
                                comm      = self%MPI_Comm_Row, &
                                !request   = request(j, k), &
                                request   = grid_handler_rcv%GRID_COMM_REQUESTS(j+dims_rcv(2)*(k-1)), &
                                ierror    = ierr(j,k))
                                !request(dims_rcv(2), dims_rcv(3)) => GRID_COMM_REQUESTS
            end do
        end do

        !call system_clocK(n)
        !if (my_rank == 0) write(*,*) "213", "Send Time:",  (n-m)

        !request(dims_rcv(2), dims_rcv(3)) => GRID_COMM_REQUESTS
        ! Deallocation of request is done in MPI_WaitAll
        !do k = 1, dims_rcv(3)
        !    call MPI_WaitAll(dims_rcv(2), request(:,k), comm_status(:,k), ierr0)
        !end do
        !ierr0 = 0
        !call MPI_WaitAll(dims_rcv(2)*dims_rcv(3), GRID_COMM_REQUESTS, GRID_COMM_STATUS, ierr0)
        !if (ierr0 /= MPI_SUCCESS) error stop "Grid Row 213 Failed"
        
        !call MPI_Comm_rank(MPI_COMM_WORLD, ierr0)
        !if (ierr0 ==0) then
            !write(*,*) "rotate 213", dims_rcv(1), dims_rcv(2), dims_rcv(3)
        !    dims_rcv = grid_handler_rcv%get_dims()
            !write(*,*) "detdims 213", dims_rcv(1), dims_rcv(2), dims_rcv(3)
        !end if 

        !call MPI_Barrier(MPI_Comm_World, ierr0)
        !call self%grid_waitall(grid_handler_rcv)
        !call MPI_Barrier(MPI_Comm_World, ierr0)

    end subroutine rotate_213

    !subroutine rotate_321()
    !    ! Body======================================================================
    !    if (dims_send(1) < grid_handler_send%overhead_factor) error stop &
    !        "Use a smaller overhead factor, the stencils go over multiple surfaces, which is not supported."

    !    do n = 0, dims_rcv(3)/grid_handler_send%overhead_factor-1
    !        do m = 1, grid_handler_send%overhead_factor
    !            call inner_loop_321()
    !        end do
    !        call MPI_WaitAll(dims_rcv(2), &
    !            grid_handler_rcv%GRID_COMM_REQUESTS(1+dims_rcv(2)*(m-1):dims_rcv(2)*m), & !request(:,m)
    !            grid_handler_rcv%GRID_COMM_STATUS(1+dims_rcv(2)*(m-1):dims_rcv(2)*m), &   !comm_status(:,m)
    !            ierr0)
    !            if (ierr0 /= MPI_SUCCESS) error stop "Grid Col 321 Failed in rotate_321"
    !    end do
    !    
    !    do m = 1, modulo(dims_rcv(3), grid_handler_send%overhead_factor)
    !        call inner_loop_321()
    !    end do
    !    call MPI_WaitAll(dims_rcv(2), &
    !            grid_handler_rcv%GRID_COMM_REQUESTS(1+dims_rcv(2)*(m-1):dims_rcv(2)*m), & !request(:,m)
    !            grid_handler_rcv%GRID_COMM_STATUS(1+dims_rcv(2)*(m-1):dims_rcv(2)*m), &   !comm_status(:,m)
    !            ierr0)

    !    if (ierr0 /= MPI_SUCCESS) error stop "Grid Col 321 Failed in rotate_321"

    !end subroutine rotate_321

    !subroutine inner_loop_321()
    !    k = m+n*grid_handler_send%overhead_factor
    !    stencil_send_gpu => work_space3D_send_gpu(:,m, 1)
    !    
    !    !$acc kernels
    !    !$acc loop independent
    !    do j = 1, dims_rcv(2)       
    !        !$acc loop independent
    !        do i = 1, dims_rcv(1)  
    !            stencil_send_gpu(i) = grid3D_pointer_send_gpu(k, j, i)
    !        end do
    !    end do
    !    !$acc end kernels

    !    do j = 1, dims_rcv(2)       
    !        call MPI_Igather(SENDBUF   = stencil_send_gpu, &
    !                        sendcount = send_count, &
    !                        sendtype  = MPI_DOUBLE, &
    !                        recvbuf   = grid3D_pointer_rcv_gpu(:, j, rcv_j(k)), &
    !                        recvcount = send_count, &
    !                        recvtype  = MPI_DOUBLE, &
    !                        root      = root(k), &
    !                        comm      = self%MPI_Comm_Column, &
    !                        request   = grid_handler_rcv%GRID_COMM_REQUESTS(j+dims_rcv(2)*(k-1)), &
    !                        ierror    = ierr(j,k))
    !    end do
    !end subroutine inner_loop_321

    end subroutine rotate_grid

end module grid_comm_gpu_module
